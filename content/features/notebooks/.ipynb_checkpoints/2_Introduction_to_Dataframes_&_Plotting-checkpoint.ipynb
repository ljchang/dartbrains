{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes ( Pandas ) and Plotting ( Matplotlib/Seaborn )\n",
    "\n",
    "*Written by Jin Cheong & Luke Chang*\n",
    "\n",
    "In this lab we are going to learn how to load and manipulate datasets in a dataframe format using Pandas   \n",
    "and create beautiful plots using Matplotlib and Seaborn. Pandas is akin to a data frame in R and provides an intuitive way to interact with data in a 2D data frame. Matplotlib is a standard plotting library that is similar in functionality to Matlab's object oriented plotting. Seaborn is also a plotting library built on the Matplotlib framework which carries useful pre-configured plotting schemes. \n",
    "\n",
    "After the tutorial you will have the chance to apply the methods to a new set of data. \n",
    "\n",
    "Also, [here is a great set of notebooks](https://github.com/jakevdp/PythonDataScienceHandbook) that also covers the topic.  In addition, here is a brief [video](https://neurohackademy.org/course/complex-data-structures/) providing a useful introduction to pandas. \n",
    "\n",
    "\n",
    "First, we load the basic packages we will be using in this tutorial.  Notice how we import the modules using an abbreviated name.  This is to reduce the amount of text we type when we use the functions.\\\n",
    "\n",
    "**Note**: `%matplotlib inline` is an example of 'cell magic' and enables plotting *within* the notebook and not opening a separate window. In addition, you may want to try using `%matplotlib notebook`, which will allow more interactive plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.142001Z",
     "start_time": "2019-04-11T11:47:39.703543Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "## Loading Data\n",
    "We use the pd.read_csv() to load a .csv file into a dataframe. \n",
    "Note that read_csv() has many options that can be used to make sure you load the data correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `pwd` will print the path of the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.156574Z",
     "start_time": "2019-04-11T11:47:41.150899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lukechang/Github/dartbrains'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:32:48.507978Z",
     "start_time": "2019-04-12T03:32:48.503985Z"
    }
   },
   "source": [
    "Pandas has many ways to read data different data formats into a dataframe.  Here we will use the 'pd.read_csv' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:30:21.748037Z",
     "start_time": "2019-04-12T03:30:21.677289Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../data/salary.csv' does not exist: b'../data/salary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4324eff8f110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/salary.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'infer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../data/salary.csv' does not exist: b'../data/salary.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/salary.csv', sep = ',', header='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always use the `?` to access the docstring for a function for more information about the inputs and general useage guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.289300Z",
     "start_time": "2019-04-11T11:47:39.700Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to check the dataframe \n",
    "There are many ways to examine your dataframe. \n",
    "One easy way is to execute the dataframe itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.290444Z",
     "start_time": "2019-04-11T11:47:39.704Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T18:35:36.660858",
     "start_time": "2017-01-16T18:35:36.656319"
    }
   },
   "source": [
    "However, often the dataframes can be large and we may be only interested in seeing the first few rows.  `df.head()` is useful for this purpose.  `shape` is another useful method for getting the dimensions of the matrix.  We will print the number of rows and columns in this data set by using output formatting.  Use the `%` sign to indicate the type of data (e.g., `%i`=integer, `%d`=float, `%s`=string), then use the `%` followed by a tuple of the values you would like to insert into the text.  See [here](https://pyformat.info/) for more info about formatting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.291868Z",
     "start_time": "2019-04-11T11:47:39.707Z"
    }
   },
   "outputs": [],
   "source": [
    "print('There are %i rows and %i columns in this data set' % df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.292914Z",
     "start_time": "2019-04-11T11:47:39.709Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the top row, you have column names, that can be called like a dictionary (a dataframe can be essentially thought of as a dictionary with column names as the keys). The left most column (0,1,2,3,4...) is called the index of the dataframe. The default index is sequential integers, but it can be set to anything as long as each row is unique (e.g., subject IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.294018Z",
     "start_time": "2019-04-11T11:47:39.714Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Indexes\")\n",
    "print(df.index)\n",
    "print(\"Columns\")\n",
    "print(df.columns)\n",
    "print(\"Columns are like keys of a dictionary\")\n",
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the values of a column by calling it directly. Double bracket returns a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.295400Z",
     "start_time": "2019-04-11T11:47:39.718Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['salary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single bracket returns a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.296848Z",
     "start_time": "2019-04-11T11:47:39.721Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call a column like an attribute if the column name is a string \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.298348Z",
     "start_time": "2019-04-11T11:47:39.724Z"
    }
   },
   "outputs": [],
   "source": [
    "df.salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create new columns to fit your needs. \n",
    "For instance you can set initialize a new column with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.299570Z",
     "start_time": "2019-04-11T11:47:39.727Z"
    }
   },
   "outputs": [],
   "source": [
    "df['pubperyear'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can create a new column pubperyear, which is the ratio of the number of papers published per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.300667Z",
     "start_time": "2019-04-11T11:47:39.730Z"
    }
   },
   "outputs": [],
   "source": [
    "df['pubperyear'] = df['publications']/df['years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.302455Z",
     "start_time": "2019-04-11T11:47:39.733Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and slicing\n",
    "Indexing in Pandas can be tricky. There are four ways to index: loc, iloc, ix, and explicit indexing(useful for booleans).  \n",
    "\n",
    "\n",
    "First, we will try using `.loc`.  This method references the explicit index. it works for both index names and also column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.304293Z",
     "start_time": "2019-04-11T11:47:39.735Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[0,['salary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wil try `.iloc`.  This method references the implicit python index (starting from 0, exclusive of last number).  You can think of this like row by column indexing using integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.305831Z",
     "start_time": "2019-04-11T11:47:39.738Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0:3,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an older method called `.ix`, which will likely eventually be phased out of pandas.  It can be useful to combine explicit and implicit indexing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T03:33:24.552874Z",
     "start_time": "2019-04-12T03:33:24.544648Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bc1e898d137e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.ix[0:3,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a new data frame with just Males and another for just Females. Notice, how we added the `.reset_index(drop=True)` method?   This is because assigning a new dataframe based on indexing another dataframe will retain the *original* index.  We need to explicitly tell pandas to reset the index if we want it to start from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.308443Z",
     "start_time": "2019-04-11T11:47:39.744Z"
    }
   },
   "outputs": [],
   "source": [
    "maledf = df[df.gender==0].reset_index(drop=True)\n",
    "femaledf = df[df.gender==1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Boolean or logical indexing is useful if you need to sort the data based on some True or False value.  \n",
    "\n",
    "For instance, who are the people with salaries greater than 90K but lower than 100K ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.309665Z",
     "start_time": "2019-04-11T11:47:39.747Z"
    }
   },
   "outputs": [],
   "source": [
    "df[ (df.salary > 90000) & (df.salary < 100000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T18:36:01.082619",
     "start_time": "2017-01-16T18:36:01.078188"
    }
   },
   "source": [
    "## Dealing with missing values\n",
    "It is easy to quickly count the number of missing values for each column in the dataset using the `isnull()` method.  One thing that is  nice about Python is that you can chain commands, which means that the output of one method can be the input into the next method.  This allows us to write intuitive and concise code.  Notice how we take the `sum()` of all of the null cases.\n",
    "\n",
    "The `isnull()` method will return a dataframe with True/False values on whether a datapoint is null or not a number (nan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.310921Z",
     "start_time": "2019-04-11T11:47:39.749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain the `.null()` and `.sum()` methods to see how many null values are added up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.312034Z",
     "start_time": "2019-04-11T11:47:39.752Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the boolean indexing once again to see the datapoints that have missing values. We chained the method `.any()` which will check if there are any True values for a given axis.  Axis=0 indicates rows, while Axis=1 indicates columns.  So here we are creating a boolean index for row where *any* column has a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.313384Z",
     "start_time": "2019-04-11T11:47:39.755Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.314552Z",
     "start_time": "2019-04-11T11:47:39.757Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you may look at where the values are not null\n",
    "# Note that indexes 18, and 24 are missing. \n",
    "df[~df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T18:36:16.705384",
     "start_time": "2017-01-16T18:36:16.701256"
    }
   },
   "source": [
    "There are different techniques for dealing with missing data.  An easy one is to simply remove rows that have any missing values using the `dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.315892Z",
     "start_time": "2019-04-11T11:47:39.762Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check to make sure the missing rows are removed.  Let's also check the new dimensions of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.318403Z",
     "start_time": "2019-04-11T11:47:39.764Z"
    }
   },
   "outputs": [],
   "source": [
    "print('There are %i rows and %i columns in this data set' % df.shape)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T18:36:43.976308",
     "start_time": "2017-01-16T18:36:43.972047"
    }
   },
   "source": [
    "## Describing the data\n",
    "We can use the `.describe()` method to get a quick summary of the continuous values of the data frame. We will `.transpose()` the output to make it slightly easier to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.320177Z",
     "start_time": "2019-04-11T11:47:39.767Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get quick summary of a pandas series, or specific column of a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.321807Z",
     "start_time": "2019-04-11T11:47:39.771Z"
    }
   },
   "outputs": [],
   "source": [
    "df.departm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating data in Groups\n",
    "One manipulation we often do is look at variables in groups. \n",
    "One way to do this is to usethe `.groupby(key)` method. \n",
    "The key is a column that is used to group the variables together. \n",
    "For instance, if we want to group the data by gender and get group means, we perform the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.323560Z",
     "start_time": "2019-04-11T11:47:39.774Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('gender').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other default aggregation methods include .count(), .mean(), .median(), .min(), .max(), .std(), .var(), and .sum()\n",
    "\n",
    "Before we move on, it looks like there were more than 2 genders specified in our data. \n",
    "This is likely an error in the data collection process so let recap on how we might remove this datapoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.325011Z",
     "start_time": "2019-04-11T11:47:39.777Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['gender']==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace original dataframe without the miscoded data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.326522Z",
     "start_time": "2019-04-11T11:47:39.779Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['gender']!=2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a corrected dataframe!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.327949Z",
     "start_time": "2019-04-11T11:47:39.782Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('gender').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another powerful tool in Pandas is the split-apply-combine method. \n",
    "For instance, let's say we also want to look at how much each professor is earning in respect to the department. \n",
    "Let's say we want to subtract the departmental mean from professor and divide it by the departmental standard deviation. \n",
    "We can do this by using the `groupby(key)` method chained with the `.transform(function)` method. \n",
    "It will group the dataframe by the key column, perform the \"function\" transformation of the data and return data in same format.\n",
    "To learn more, see link [here](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.329511Z",
     "start_time": "2019-04-11T11:47:39.785Z"
    }
   },
   "outputs": [],
   "source": [
    "# key: We use the departm as the grouping factor. \n",
    "key = df['departm']\n",
    "\n",
    "# Let's create an anonmyous function for calculating zscores using lambda:\n",
    "# We want to standardize salary for each department.\n",
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "\n",
    "# Now let's calculate zscores separately within each department\n",
    "transformed = df.groupby(key).transform(zscore)\n",
    "df['salary_in_departm'] = transformed['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have `salary_in_departm` column showing standardized salary per department.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.331141Z",
     "start_time": "2019-04-11T11:47:39.788Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining datasets : pd.concat\n",
    "Recall that we sliced the dataframes into male and female dataframe in 2.3 Indexing and Slicing. Now we will learn how to put dataframes together which is done by the pd.concat method. Note how the index of this output retains the old index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.332550Z",
     "start_time": "2019-04-11T11:47:39.791Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([femaledf,maledf],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reset the index to start at zero using the .reset_index() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.334391Z",
     "start_time": "2019-04-11T11:47:39.795Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([maledf,femaledf],axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting in pandas\n",
    "Before we move into Matplotlib, here are a few plotting methods already implemented in Pandas. \n",
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.335987Z",
     "start_time": "2019-04-11T11:47:39.797Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['salary','gender']].boxplot(by='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.337699Z",
     "start_time": "2019-04-11T11:47:39.801Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['salary','years']].plot(kind='scatter', x='years', y='salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Categorical Variables. Replacing variables with .map\n",
    "If we want to plot department on the x-axis, Pandas plotting functions won't know what to do\n",
    "because they don't know where to put bio or chem on a numerical x-axis. \n",
    "Therefore one needs to change them to numerical variable to plot them with basic functionalities (we will later see how Seaborn sovles this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.339225Z",
     "start_time": "2019-04-11T11:47:39.803Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a new numericalSeries called dept_num for visualization.\n",
    "df['dept_num'] = 0\n",
    "df.loc[:,['dept_num']] = df.departm.map({'bio':0, 'chem':1,'geol':2,'neuro':3,'stat':4,'physics':5,'math':6})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.340843Z",
     "start_time": "2019-04-11T11:47:39.805Z"
    }
   },
   "outputs": [],
   "source": [
    "## Now plot all four categories\n",
    "f, axs = plt.subplots(1, 4, sharey=True)\n",
    "f.suptitle('Salary in relation to other variables')\n",
    "df.plot(kind='scatter', x='gender', y='salary', ax=axs[0], figsize=(15, 4))\n",
    "df.plot(kind='scatter', x='dept_num', y='salary', ax=axs[1])\n",
    "df.plot(kind='scatter', x='years', y='salary', ax=axs[2])\n",
    "df.plot(kind='scatter', x='age', y='salary', ax=axs[3])\n",
    "# The problem is that it treats department as a continuous variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating bar - errorbar plots in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.342305Z",
     "start_time": "2019-04-11T11:47:39.808Z"
    }
   },
   "outputs": [],
   "source": [
    "means = df.groupby('gender').mean()['salary']\n",
    "errors = df.groupby('gender').std()['salary'] / np.sqrt(df.groupby('gender').count()['salary'])\n",
    "ax = means.plot.bar(yerr=errors,figsize=(5,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib\n",
    "\n",
    "Matplotlib is an object oriented plotting library in Python that is loosely based off of plotting in matlab.  It is the primary library that many other packages build on. Here is a very concise and helpful [introduction](https://github.com/neurohackademy/visualization-in-python/blob/master/visualization-in-python.ipynb) to plotting in Python.\n",
    "Learn other matplotlib tutorials [here](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb)\n",
    "\n",
    "## create a basic lineplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.343886Z",
     "start_time": "2019-04-11T11:47:39.810Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(range(0,10),np.sqrt(range(0,10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a basic scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.345240Z",
     "start_time": "2019-04-11T11:47:39.812Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.scatter(df.salary,df.age,color='b',marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify different aspects of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.346823Z",
     "start_time": "2019-04-11T11:47:39.814Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.subplots allows you to control different aspects of multiple plots\n",
    "f,ax = plt.subplots(1,1,figsize=(4,2)) \n",
    "ax.scatter(df.salary,df.age,color='k',marker='o')\n",
    "# Setting limits on axes\n",
    "ax.set_xlim([40000,120000])\n",
    "ax.set_ylim([20,70])\n",
    "# Changing tick labels\n",
    "ax.set_xticklabels([str(int(tick)/1000)+'K' for tick in ax.get_xticks()])\n",
    "# changing label names\n",
    "ax.set_xlabel('salary')\n",
    "ax.set_ylabel('age')\n",
    "# changing the title\n",
    "ax.set_title('Scatterplot of age and salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.348009Z",
     "start_time": "2019-04-11T11:47:39.816Z"
    }
   },
   "outputs": [],
   "source": [
    "# save figure\n",
    "f.savefig('MyFirstPlot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create multiple plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.349448Z",
     "start_time": "2019-04-11T11:47:39.818Z"
    }
   },
   "outputs": [],
   "source": [
    "f,axs = plt.subplots(1,2,figsize=(15,5)) # create a plot figure, specify the size and number of figures.\n",
    "axs[0].scatter(df.age,df.salary,color='k',marker='o')\n",
    "axs[0].set_ylim([40000,120000])\n",
    "axs[0].set_xlim([20,70])\n",
    "axs[0].set_yticklabels([str(int(tick)/1000)+'K' for tick in axs[0].get_yticks()])\n",
    "axs[0].set_ylabel('salary')\n",
    "axs[0].set_xlabel('age')\n",
    "axs[0].set_title('Scatterplot of age and salary')\n",
    "\n",
    "axs[1].scatter(df.publications,df.salary,color='k',marker='o')\n",
    "axs[1].set_ylim([40000,120000])\n",
    "axs[1].set_xlim([20,70])\n",
    "axs[1].set_yticklabels([str(int(tick)/1000)+'K' for tick in axs[1].get_yticks()])\n",
    "\n",
    "axs[1].set_ylabel('salary')\n",
    "axs[1].set_xlabel('publications')\n",
    "axs[1].set_title('Scatterplot of publication and salary')\n",
    "\n",
    "f.suptitle('Scatterplots of salary and other factors')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn\n",
    "Seaborn is a plotting library built on Matplotlib that has many pre-configured plots that are often used for visualization. \n",
    "Other great tutorials about seaborn are [here](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.14-Visualization-With-Seaborn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.350980Z",
     "start_time": "2019-04-11T11:47:39.821Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.regplot(df.age,df.salary)\n",
    "ax.set_title('Salary and age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.352793Z",
     "start_time": "2019-04-11T11:47:39.823Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot(\"age\", \"salary\", data=df, kind='reg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor plots \n",
    "Factor plots allow you to visualize the distribution of parameters in different forms such as point, bar, or violin graphs.  \n",
    "\n",
    "Here are some possible values for kind : {point, bar, count, box, violin, strip}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.354631Z",
     "start_time": "2019-04-11T11:47:39.825Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(x='departm', y='salary', hue='gender', data=df, ci=68 ,kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap plots \n",
    "Heatmap plots allow you to visualize matrices such as correlation matrices that show relationships across multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T11:47:41.356496Z",
     "start_time": "2019-04-11T11:47:39.827Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df[['salary', 'years', 'age', 'publications']].corr(),annot=True,linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises ( Homework)\n",
    "The following exercises uses the dataset \"salary_exercise.csv\" adapted from material available [here](http://data.princeton.edu/wws509/datasets/#salary)\n",
    "\n",
    "These are the salary data used in Weisberg's book, consisting of observations on six variables for 52 tenure-track professors in a small college. The variables are:\n",
    "\n",
    "- sx = Sex, coded 1 for female and 0 for male\n",
    "- rk = Rank, coded\n",
    " - 1 for assistant professor,\n",
    " - 2 for associate professor, and\n",
    " - 3 for full professor\n",
    "- yr = Number of years in current rank\n",
    "- dg = Highest degree, coded 1 if doctorate, 0 if masters\n",
    "- yd = Number of years since highest degree was earned\n",
    "- sl = Academic year salary, in dollars.\n",
    "\n",
    "Reference: S. Weisberg (1985). Applied Linear Regression, Second Edition. New York: John Wiley and Sons. Page 194.\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "Read the salary_exercise.csv into a dataframe, and change the column names to a more readable format such as \n",
    "sex, rank, yearsinrank, degree, yearssinceHD, and salary.   \n",
    "Clean the data by excluding rows with any missing value. \n",
    "What are the overall mean, standard deviation, min, and maximum of professors' salary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-18T12:11:07.688401",
     "start_time": "2017-01-18T12:11:07.662760"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Using the same data, what are the means and standard deviations of salary for different professor ranks?   \n",
    "Create a new column on the original dataframe in which you calculate the standardized salary for each \"rank\" group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Recreate the plot shown in figure.   \n",
    "On the left is a correlation of all parameters of only the male professors.  \n",
    "On the right is the same but only for female professors.   \n",
    "The colormap code used is 'RdBu_r'. Read the Docstrings on sns.heatmap or search the internet to figure out how to change the colormap, scale the colorbar, and create square line boundaries.   \n",
    "Place titles for each plot as shown, and your name as the main title.   \n",
    "\n",
    "![](../images/labs/plotting/hw2-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T23:41:49.021245",
     "start_time": "2017-01-16T23:41:49.018846"
    }
   },
   "source": [
    "## Exercise 4\n",
    "Recreate the following plot from the salary_exercise.csv dataset.   \n",
    "Create a 1 x 2 subplot.   \n",
    "On the left is a bar-errorbar of salary per gender.   \n",
    "On the right is a scatterplot of salary on y-axis and years in rank on the x-axis.  \n",
    "Set the axis limits as shown in the picture and modify their lables.   \n",
    "Add axis label names.   \n",
    "Add a legend for the scatterplot and place it at a bottom-right location.  \n",
    "Add your name as the main title of the plot.   \n",
    "\n",
    "![](../images/labs/plottinghw2-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "44px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
